{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Model_Third_Attempt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Yk33mywbpcxd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PyTorch Third Attempt"
      ]
    },
    {
      "metadata": {
        "id": "2JLf_jcWAi26",
        "colab_type": "code",
        "outputId": "94e3b5e6-361d-49f9-93c8-1035b08b0263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "# # Check out available CPU and GPU memory\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "def print_CPU_GPU_info(GPUs):\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(f\"\\nCPU \\tRAM Free: {humanize.naturalsize(psutil.virtual_memory().available)}\"\n",
        "          f\"    | Proc size: {humanize.naturalsize(process.memory_info().rss)}\")\n",
        "    if GPUs[0]: \n",
        "        for i,gpu in enumerate(GPUs):\n",
        "            print(f\"GPU {i} \\tRAM Free: {gpu.memoryFree/1000:.3f} GB  \"\n",
        "                  f\"| Used: {gpu.memoryUsed/1000:.3f} GB\"\n",
        "                  f\"\\t| Utilization: {gpu.memoryUtil*100:3.0f}% | \"\n",
        "                  f\"Total Memory: {gpu.memoryTotal/1000:.3f} GB\")\n",
        "    else: print(f\"Not on a GPU\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tl7BZKJuOQ4t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1ecb979b-dbfd-4aa2-b94f-5413df46646a"
      },
      "cell_type": "code",
      "source": [
        "print_CPU_GPU_info(GPU.getGPUs())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CPU \tRAM Free: 12.9 GB    | Proc size: 143.1 MB\n",
            "GPU 0 \tRAM Free: 11.441 GB  | Used: 0.000 GB\t| Utilization:   0% | Total Memory: 11.441 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OgjeP1M8pcxe",
        "colab_type": "code",
        "outputId": "f3e820af-9e20-4a48-862f-b24f1d065924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "IPython.OutputArea.prototype._should_scroll = function(lines) { return false; }\n",
        "// disable scrollable cells"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.OutputArea.prototype._should_scroll = function(lines) { return false; }\n",
              "// disable scrollable cells"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "iX8Q9D1Fpcxj",
        "colab_type": "code",
        "outputId": "3113889f-0cd8-4953-cd10-5440477d5bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/samryan18/chess-ray-vision\n",
        "! git clone https://github.com/mukundv7/crvdataset\n",
        "! mv chess-ray-vision/clean_notebooks/* .\n",
        "! mkdir train_full\n",
        "! mv crvdataset/chess-positions/train-full/* train_full/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'chess-ray-vision'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 80 (delta 11), reused 77 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (80/80), done.\n",
            "Cloning into 'crvdataset'...\n",
            "remote: Enumerating objects: 39509, done.\u001b[K\n",
            "remote: Counting objects: 100% (39509/39509), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39378/39378), done.\u001b[K\n",
            "remote: Total 39509 (delta 150), reused 39484 (delta 128), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (39509/39509), 770.54 MiB | 32.50 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n",
            "Checking out files: 100% (39691/39691), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dS7DVuMHQQ-S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ! mkdir train_mini\n",
        "# ! mv crvdataset/chess-positions/train-mini/* ./train_mini\n",
        "# ! mkdir train_mini/train\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVc5YZiHpcxl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup Stuff"
      ]
    },
    {
      "metadata": {
        "id": "G5YloGGWqY72",
        "colab_type": "code",
        "outputId": "e86fede4-242e-4756-8e6e-d08038bcd19b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "# Pytorch Colab Setup\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "  \n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.0.1 from https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl (614.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 614.8MB 27kB/s \n",
            "\u001b[31mfastai 1.0.50.post1 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.0.1.post2\n",
            "    Uninstalling torch-1.0.1.post2:\n",
            "      Successfully uninstalled torch-1.0.1.post2\n",
            "Successfully installed torch-1.0.1\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "joxmigYl9iK9",
        "colab_type": "code",
        "outputId": "0130cc63-f1dd-4e7b-fa15-ba31aef1d166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "cell_type": "code",
      "source": [
        "## Required packages (Install in Colab)\n",
        "!pip install tensorflow\n",
        "!pip install scipy\n",
        "!pip install numpy\n",
        "!pip install Pillow\n",
        "!pip install image"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.7)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.9)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.6)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (40.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.1)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.1.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.14.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow) (0.46)\n",
            "Requirement already satisfied: image in /usr/local/lib/python3.6/dist-packages (1.5.27)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from image) (4.1.1)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.6/dist-packages (from image) (2.1.7)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->image) (0.46)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wkGhAHeKpcxx",
        "colab_type": "code",
        "outputId": "7fbd7a82-6aa4-450b-98cc-2eb7a282ff41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np;\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time, datetime\n",
        "from pytorch_general.pytorch_helper import imshow\n",
        "from pytorch_general.tensorboard_helper import Logger\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from random import randint\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "from typing import Callable\n",
        "import torch\n",
        "import dill\n",
        "import torch.optim as optim\n",
        "device =torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "OIq7Yn4mpmoD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "piece_symbols = 'prbnkqPRBNKQ'\n",
        "\n",
        "def onehot_from_fen(fen):\n",
        "    eye = np.eye(13)\n",
        "    output = np.empty((0, 13))\n",
        "    fen = re.sub('[-]', '', fen)\n",
        "\n",
        "    for char in fen:\n",
        "        if(char in '12345678'):\n",
        "            output = np.append(\n",
        "              output, np.tile(eye[12], (int(char), 1)), axis=0)\n",
        "        else:\n",
        "            idx = piece_symbols.index(char)\n",
        "            output = np.append(output, eye[idx].reshape((1, 13)), axis=0)\n",
        "\n",
        "    return output\n",
        "\n",
        "def fen_from_onehot(one_hot):\n",
        "    output = ''\n",
        "    for j in range(8):\n",
        "        for i in range(8):\n",
        "            if(one_hot[j][i] == 12):\n",
        "                output += ' '\n",
        "            else:\n",
        "                output += piece_symbols[one_hot[j][i]]\n",
        "        if(j != 7):\n",
        "            output += '-'\n",
        "\n",
        "    for i in range(8, 0, -1):\n",
        "        output = output.replace(' ' * i, str(i))\n",
        "\n",
        "    return output\n",
        "\n",
        "class_prob = onehot_from_fen('4kN1N-B1P5-QQ3B2-R1n1b3-8-1p2P3-1K6-6b1')\n",
        "\n",
        "# one_hot = np.zeros((64, 13))\n",
        "# one_hot[np.arange(64,13), class_labels] = 1\n",
        "# # class_labels\n",
        "# np.shape(class_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IvOPDfgUpcx0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QkeM2qiepcx3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# xz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1uHyAjRIpcx5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AzTlMw2xpcx_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_batch(directory='train_full/', batch_size=32):\n",
        "    '''\n",
        "    Probably a better way to do this using something like this:\n",
        "    https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n",
        "    \n",
        "    This loads a single random batch from the training set.\n",
        "    '''\n",
        "    pathlist = list(Path(directory).glob('**/*.jpeg'))\n",
        "    labels = []\n",
        "    images = []\n",
        "    n_files = len(pathlist)\n",
        "    random_indicies = [randint(0, n_files) for _ in range(batch_size)]\n",
        "\n",
        "    for path in [pathlist[x] for x in random_indicies]:\n",
        "        label = str(path).split(directory)[1].split(f'.')[0]\n",
        "        label = onehot_from_fen(label)\n",
        "\n",
        "        img = np.asarray(Image.open(str(path))).astype('uint8')\n",
        "        labels.append(label)\n",
        "        images.append(img)\n",
        "        \n",
        "    test_images, test_labels = (images, labels) # TODO\n",
        "        \n",
        "    return images, labels, test_images, test_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CustomChessDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, transform=None, root='train_full/', train=True):\n",
        "#         train_images, train_labels, test_images, test_labels = load_datasets()\n",
        "        self.transform = transform\n",
        "\n",
        "        self._train = train\n",
        "            \n",
        "        self.root = root\n",
        "        self.pathlist = list(Path(self.root).glob('**/*.jpeg'))\n",
        "        self.n_files = len(self.pathlist)\n",
        "\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_files\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.pathlist[idx]\n",
        "        label = str(path).split(self.root)[1].split(f'.')[0]\n",
        "        label = onehot_from_fen(label)\n",
        "        img = np.asarray(Image.open(str(path))).astype('uint8')\n",
        "\n",
        "        img_as_img = Image.fromarray(img)\n",
        "        img_as_img = img_as_img.convert('L')\n",
        "        img_as_img = self.transform(img_as_img)\n",
        "        \n",
        "        \n",
        "#         _,class_labels = torch.max(self.to_tensor(self.labels[idx]).long().to(device),1) \n",
        "\n",
        "        return (self.to_tensor(img_as_img), \n",
        "                label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jprHKvxU8D0M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# NEW FROM KERAS VERSION\n",
        "\n",
        "import glob\n",
        "from random import shuffle\n",
        "from skimage.util.shape import view_as_blocks\n",
        "from skimage import transform as sktransform\n",
        "from skimage import io\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "piece_symbols = 'prbnkqPRBNKQ'\n",
        "\n",
        "def fen_from_filename(filename):\n",
        "    base = os.path.basename(filename)\n",
        "    return os.path.splitext(base)[0]\n",
        "\n",
        "def process_image(img):\n",
        "    downsample_size = 200\n",
        "    square_size = int(downsample_size/8)\n",
        "    img_read = io.imread(img)\n",
        "    img_read = sktransform.resize(\n",
        "      img_read, (downsample_size, downsample_size), mode='constant')\n",
        "    tiles = view_as_blocks(img_read, block_shape=(square_size, square_size, 3))\n",
        "    tiles = tiles.squeeze(axis=2)\n",
        "    return tiles.reshape(64, square_size, square_size, 3)\n",
        "\n",
        "# def train_gen(features, labels, batch_size):\n",
        "#     for i, img in enumerate(features):\n",
        "#         y = onehot_from_fen(fen_from_filename(img))\n",
        "#         x = process_image(img)\n",
        "#         yield x, y\n",
        "\n",
        "# def pred_gen(features, batch_size):\n",
        "#     for i, img in enumerate(features):\n",
        "#         yield process_image(img)\n",
        "        \n",
        "\n",
        "class CustomChessDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, transform=None, root='train_full/', train=True):\n",
        "#         train_images, train_labels, test_images, test_labels = load_datasets()\n",
        "\n",
        "        self._train = train\n",
        "            \n",
        "        self.root = root\n",
        "        self.pathlist = list(Path(self.root).glob('**/*.jpeg'))\n",
        "        self.n_files = len(self.pathlist)\n",
        "\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        \n",
        "        self.train_size = 10000\n",
        "        self.test_size = 3000\n",
        "\n",
        "        self.train = glob.glob(\"train_full/*.jpeg\")\n",
        "        self.test = glob.glob(\"train_full/*.jpeg\")\n",
        "\n",
        "        shuffle(self.train)\n",
        "        shuffle(self.test)\n",
        "\n",
        "        self.train = self.train[:self.train_size]\n",
        "        self.test = self.test[:self.test_size]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.train[idx]\n",
        "        label = onehot_from_fen(fen_from_filename(img))\n",
        "        img_as_img = process_image(img)\n",
        "\n",
        "        \n",
        "        \n",
        "#         _,class_labels = torch.max(self.to_tensor(self.labels[idx]).long().to(device),1) \n",
        "\n",
        "        return ((torch.from_numpy(img_as_img).float()), \n",
        "                label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eyai4agLpcyB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define a transform to normalize the data\n",
        "batch_size=10 # this needs to be small ish bc bigger models will scale memory usage exponentially\n",
        "downsample_size=160\n",
        "transform = transforms.Compose([transforms.Resize(downsample_size)])\n",
        "\n",
        "train_dataset = CustomChessDataset(root='train_full/', train=True, transform=transform)\n",
        "test_dataset = CustomChessDataset(root='train_full/', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# print(f'normalized image example-downsampled to {downsample_size}x{downsample_size}')\n",
        "# image, label = next(iter(test_loader))\n",
        "# print(np.shape(image))\n",
        "# imshow(image[0,:]);\n",
        "# image, label = next(iter(test_loader))\n",
        "\n",
        "# imshow(image[0,:]);\n",
        "# image, label = next(iter(test_loader))\n",
        "\n",
        "# imshow(image[0,:]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L84LWc3jpcyL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model: nn.Module, \n",
        "                log_dir: str,\n",
        "                train_loader,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                num_epochs,\n",
        "               log_freq,\n",
        "               max_per_epoch=-1) -> nn.Module:\n",
        "    t = datetime.datetime.now()\n",
        "    now = time.mktime(t.timetuple()) - 1550000000\n",
        "    logger = Logger(f'{log_dir} ({now})/')\n",
        "    \n",
        "    print(now)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    total_step = len(train_loader)\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}')\n",
        "        running_loss = 0\n",
        "        step = 0\n",
        "#         for step, (images, labels) in tqdm_notebook(enumerate(train_loader), total=len(train_loader), unit=\"mini-batches\"):\n",
        "        for step, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "            images, labels = images.to(device), labels.long().to(device)\n",
        "\n",
        "\n",
        "            output = model(images).to(device)\n",
        "#             print(images.size())\n",
        "            _,class_labels = torch.max(labels,1) \n",
        "#             print(class_labels.size())\n",
        "#             print(output.size())\n",
        "            loss = criterion(output, class_labels).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute accuracy\n",
        "            _, argmax = torch.max(output, 1)\n",
        "            accuracy = float((class_labels == argmax.squeeze()).float().mean().cpu())\n",
        "#             print(f'Accuracy: {accuracy}')\n",
        "\n",
        "            running_loss += float(loss.item())\n",
        "            \n",
        "            del(images)\n",
        "            del(labels)\n",
        "            del(class_labels)\n",
        "            \n",
        "            if step % log_freq == 0:\n",
        "\n",
        "                overall_step = epoch*total_step + step\n",
        "\n",
        "                # 1. Log scalar values (scalar summary)\n",
        "                info = { 'loss': loss.item(), 'accuracy': accuracy }\n",
        "\n",
        "                for key, value in info.items():\n",
        "                    logger.scalar_summary(key, value, overall_step)\n",
        "\n",
        "                # 2. Log values and gradients of the parameters (histogram summary)\n",
        "                for key, value in model.named_parameters():\n",
        "                    key = key.replace('.', '/')\n",
        "                    logger.histo_summary(key, value.data.cpu().numpy(), overall_step)\n",
        "                    try:\n",
        "                        logger.histo_summary(key+'/grad', value.grad.data.cpu().numpy(), overall_step)\n",
        "                    except (AttributeError):\n",
        "                        # During transfer learning some of the variables don't have grads\n",
        "                        pass\n",
        "            \n",
        "            if max_per_epoch > 0 and step > max_per_epoch:\n",
        "                break\n",
        "        \n",
        "        print(f\"{epoch}: Training loss: {running_loss/len(train_loader)}\")\n",
        "        print(f\"{epoch}: Training accuracy: {accuracy}\")\n",
        "#         print(class_labels)\n",
        "#         print(argmax)\n",
        "\n",
        " \n",
        "    return model\n",
        "\n",
        "\n",
        "def test_model(model, criterion, test_loader) -> float:\n",
        "    model = model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    accuracies = []\n",
        "    losses = []\n",
        "    total_step = len(test_loader)\n",
        "    with torch.no_grad():\n",
        "        for i in range(total_step):\n",
        "            for  images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.long().to(device)\n",
        "                _,class_labels = torch.max(labels,1) \n",
        "\n",
        "                output = model(images).to(device)\n",
        "                loss = criterion(output, class_labels)\n",
        "                losses.append(float(loss.item()))\n",
        "\n",
        "\n",
        "                # Compute accuracy\n",
        "                _, argmax = torch.max(output, 1)\n",
        "                accuracy = float((class_labels == argmax.squeeze()).float().mean().cpu())\n",
        "                accuracies.append(accuracy)\n",
        "                \n",
        "    print(f'Accuracy of the network on test images: {np.average(accuracies)}')\n",
        "    print(f'Avg. Loss of the network on test images: {np.average(losses)}')\n",
        "\n",
        "    return np.average(accuracies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rWBnIsv7ltP3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7K27IuX2pcyP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dB1n80s8nXLt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRQAmGX57yo0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1) \n",
        "\n",
        "class SimpleCNN(torch.nn.Module):\n",
        "    def __init__(self, batch_size):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.name = 'SimpleCNN'\n",
        "        self.batch_size=batch_size\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1),\n",
        "            nn.LeakyReLU(negative_slope=0.1))\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1),\n",
        "            nn.ReLU())\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1),\n",
        "            nn.ReLU())\n",
        "        self.flatten = Flatten()\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(32*19*19, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p = 0.1))\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(256, 13))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(self.batch_size*64,3,25,25)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        \n",
        "        x = self.flatten(x)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "#         print(f'xsize: {x.size()}')\n",
        "        x = x.reshape(self.batch_size,64,13)\n",
        "\n",
        "        return(x)\n",
        "    \n",
        "class BiggerCNN(torch.nn.Module):\n",
        "    def __init__(self, batch_size):\n",
        "        super(BiggerCNN, self).__init__()\n",
        "        self.name = 'BiggerCNN'\n",
        "        self.batch_size=batch_size\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.LeakyReLU(negative_slope=0.1))\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU())\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU())\n",
        "        self.flatten = Flatten()\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(64*19*19, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p = 0.1))\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p = 0.1))\n",
        "        self.fc3 = nn.Sequential(\n",
        "            nn.Linear(256, 13))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(self.batch_size*64,3,25,25)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        \n",
        "        x = self.flatten(x)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "#         print(f'xsize: {x.size()}')\n",
        "        x = x.reshape(self.batch_size,64,13)\n",
        "\n",
        "        return(x)\n",
        "    \n",
        "class BatchNormBiggerCNN(torch.nn.Module):\n",
        "    def __init__(self, batch_size):\n",
        "        super(BatchNormBiggerCNN, self).__init__()\n",
        "        self.name = 'BatchNormBiggerCNN'\n",
        "        self.batch_size=batch_size\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(negative_slope=0.1),\n",
        "            nn.Dropout(p = 0.1))\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p = 0.1))\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU())\n",
        "        self.flatten = Flatten()\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(64*19*19, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p = 0.1))\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p = 0.1))\n",
        "        self.fc3 = nn.Sequential(\n",
        "            nn.Linear(256, 13))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(self.batch_size*64,3,25,25)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        \n",
        "        x = self.flatten(x)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "#         print(f'xsize: {x.size()}')\n",
        "        x = x.reshape(self.batch_size,64,13)\n",
        "\n",
        "        return(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QaHWGkUssACN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from torchsummary import summary\n",
        "\n",
        "# summary(net, input_size=(64, 32, 25, 25))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KxR7xzEFMaPR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9a510599-f420-4fd4-fccc-4447bfa79665"
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ngrok already installed\n",
            "Tensorboard Link: http://aec27de6.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7c-HvzxJMddu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2W2kijdBpcyV",
        "colab_type": "code",
        "outputId": "2d821e92-0f56-471f-ef74-ef44d5b64078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "log_freq=20\n",
        "\n",
        "# 0.0005 best so far\n",
        "learning_rates = [ 0.0004, 0.0002, 0.0007]\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "    \n",
        "    net = BiggerCNN(batch_size=batch_size)\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "    log_dir = f'./logs/{net.name}_lr{learning_rate}'\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    model = train_model(net,\n",
        "                    log_dir,\n",
        "                    train_loader,\n",
        "                    criterion,\n",
        "                    optimizer,\n",
        "                    num_epochs, \n",
        "                    log_freq,\n",
        "                    max_per_epoch=-1) # max per epoch is a debugging thing\n",
        "\n",
        "# final_acc = test_model(model, criterion, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4252158.0\n",
            "Epoch 0\n",
            "0: Training loss: 1.2905190806314348\n",
            "0: Training accuracy: 0.9769230484962463\n",
            "Epoch 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NnLTBOOrpcyr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "24fb72de-31f1-4e45-b347-c5a307a97771"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-3a3cf6efc882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m for rec in tqdm_notebook(items, \n\u001b[0;32m----> 8\u001b[0;31m                          \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                          desc=\"Processing records\"):\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# any code processing the elements in the iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'total' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "E2lxZFKlsUOR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZOU7Z3gxpcyt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Za8WUOnFHWEu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MJAB_gjrHWJ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1aWhV1_HWRR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# total_step = len(train_loader)\n",
        "# num_epochs=10\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0\n",
        "#     for step, (images, labels) in enumerate(train_loader):\n",
        "#         images, labels = images.to(device), labels.long().to(device)\n",
        "#         imshow(images[0].cpu())\n",
        "\n",
        "        \n",
        "# #         print(np.shape(labels))\n",
        "\n",
        "#         _,class_labels = torch.max(labels,1) \n",
        "#         print(class_labels[0])\n",
        "\n",
        "#         accuracy = (class_labels == class_labels).float().mean()\n",
        "\n",
        "#         running_loss += 1\n",
        "\n",
        "\n",
        "#     print_CPU_GPU_info(GPU.getGPUs())\n",
        "\n",
        "#     print(f\"{epoch}: Training loss: {running_loss/len(train_loader)}\")\n",
        "#     print(f\"{epoch}: Training accuracy: {accuracy}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TtBdqd5rpcyx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class SimpleCNN(torch.nn.Module):\n",
        "    \n",
        "    #Our batch shape for input x is (3, 32, 32)\n",
        "    \n",
        "    def __init__(self, batch_size):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.batch_size=batch_size\n",
        "        \n",
        "        #Input channels = 3, output channels = 18\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=36, kernel_size=20, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(36),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4, padding=0),\n",
        "            nn.Dropout(p = 0.1),\n",
        "            nn.ReLU())\n",
        "        \n",
        "        \n",
        "        #4608 input features, 64 output features (see sizing flow below)\n",
        "        # 200/2, 200/2\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(44100, 64*13),\n",
        "            nn.ReLU())\n",
        "        \n",
        "#         self.fc2 = nn.Sequential(\n",
        "#             nn.Linear(64*26, 64*13),\n",
        "#             nn.ReLU())\n",
        "        \n",
        "        self.fc3 = nn.Sequential(\n",
        "            nn.Linear(64*13, 64*13),\n",
        "            nn.ReLU())\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        #Reshape data to input to the input layer of the neural net\n",
        "        #-1 infers this dimension from the other given dimension\n",
        "#         print(x.size())\n",
        "        x = x.view(-1, 44100)\n",
        "#         x = x.view(-1, 36*40*40)\n",
        "    \n",
        "#         print(x.size())\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "#         x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        \n",
        "        # Reshape data to output shape\n",
        "        out = x.reshape(self.batch_size, 64, 13)\n",
        "\n",
        "        return(out)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "# class BetterCNN(torch.nn.Module):\n",
        "    \n",
        "#     #Our batch shape for input x is (3, 32, 32)\n",
        "    \n",
        "#     def __init__(self, batch_size):\n",
        "#         super(BetterCNN, self).__init__()\n",
        "#         self.batch_size=batch_size\n",
        "        \n",
        "#         #Input channels = 3, output channels = 18\n",
        "        \n",
        "#         self.conv1 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=1, out_channels=18, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(18),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "#             nn.Dropout(p = 0.1),\n",
        "#             nn.ReLU())\n",
        "#         self.conv2 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=18, out_channels=16, kernel_size=1, stride=3),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.MaxPool2d(kernel_size=4, stride=4, padding=0),\n",
        "#             nn.Dropout(p = 0.1))\n",
        "        \n",
        "#         #4608 input features, 64 output features (see sizing flow below)\n",
        "#         # 200/2, 200/2\n",
        "#         self.fc1 = nn.Sequential(\n",
        "#             nn.Linear(32768, 1024),\n",
        "#             nn.ReLU())\n",
        "        \n",
        "#         self.fc2 = nn.Sequential(\n",
        "#             nn.Linear(1024, 64*13))\n",
        "\n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.relu(self.conv2(x))\n",
        "\n",
        "        \n",
        "#         #Reshape data to input to the input layer of the neural net\n",
        "#         #-1 infers this dimension from the other given dimension\n",
        "#         x = x.view(-1, 32768)\n",
        "        \n",
        "#         x = self.fc1(x)\n",
        "#         x = self.fc2(x)\n",
        "        \n",
        "#         # Reshape data to output shape\n",
        "#         out = x.reshape(self.batch_size, 64, 13)\n",
        "\n",
        "#         return(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPlLCnefpcyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class Net(nn.Module):\n",
        "#     def __init__(self, batch_size):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.batch_size = batch_size\n",
        "#         self.fc1 = nn.Sequential(\n",
        "#             nn.Linear(200*200*1, 4096),\n",
        "#             nn.ReLU())\n",
        "#         self.fc2 = nn.Sequential(\n",
        "#             nn.Linear(4096, 2048),\n",
        "#             nn.ReLU())\n",
        "#         self.fc3 = nn.Sequential(\n",
        "#             nn.Linear(2048, 1024),\n",
        "#             nn.ReLU())\n",
        "#         self.fc4 = nn.Sequential(\n",
        "#             nn.Linear(1024, 64*13),\n",
        "#             nn.ReLU())\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.view(x.shape[0], -1)\n",
        "#         out = self.fc1(x)\n",
        "#         out = self.fc2(out)\n",
        "#         out = self.fc3(out)\n",
        "#         out = self.fc4(out)\n",
        "#         out = out.reshape(1, 64, 13)\n",
        "#         return out\n",
        "\n",
        "# class TinyNet(nn.Module):\n",
        "#     def __init__(self, batch_size):\n",
        "#         super(TinyNet, self).__init__()\n",
        "#         self.batch_size = batch_size\n",
        "#         self.fc1 = nn.Sequential(\n",
        "#             nn.Linear(200*200*1, 64*13),\n",
        "#             nn.ReLU())\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.view(x.shape[0], -1)\n",
        "#         out = self.fc1(x)\n",
        "#         out = out.reshape(self.batch_size, 64, 13)\n",
        "#         return out\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "# class CNN_1(nn.Module):\n",
        "#     def __init__(self, batch_size):\n",
        "#         super(CNN_1, self).__init__()\n",
        "#         self.batch_size = batch_size\n",
        "#         self.conv1 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1),\n",
        "#             nn.BatchNorm2d(32),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "#             nn.Dropout(p = 0.1))\n",
        "#         self.conv2 = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=32, out_channels=8, kernel_size=1, stride=3),\n",
        "#             nn.BatchNorm2d(8),\n",
        "#             nn.MaxPool2d(kernel_size=12, stride=12, padding=0),\n",
        "#             nn.Dropout(p = 0.1))\n",
        "#         self.fc1 = nn.Sequential(\n",
        "#             nn.Linear(1024, 64*13),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(p = 0.1))\n",
        "        \n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.view(x.shape[0], -1)\n",
        "#         out = self.conv1(x)\n",
        "#         out = self.conv2(out)\n",
        "#         out = out.view(out.size(0), -1)\n",
        "#         out = self.fc1(out)\n",
        "#         out = out.reshape(self.batch_size, 64, 13)\n",
        "\n",
        "#         return out\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "knTCzc3Gpcy9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # def load_datasets(directory='train_full/'):\n",
        "# #     pathlist = Path(directory).glob('**/*.jpeg')\n",
        "# #     labels = []\n",
        "# #     images = []\n",
        "# #     for path in pathlist:\n",
        "# #         label = str(path).split(directory)[1].split(f'.')[0]\n",
        "# #         label = onehot_from_fen(label)\n",
        "\n",
        "# #         img = np.asarray(Image.open(str(path))).astype('uint8')\n",
        "# #         labels.append(label)\n",
        "# #         images.append(img)\n",
        "        \n",
        "# #     test_images, test_labels = (images, labels) # TODO\n",
        "        \n",
        "# #     return images, labels, test_images, test_labels\n",
        "# # images, labels, test_images, test_labels = load_datasets()\n",
        "\n",
        "\n",
        "# def load_batch(directory='train_full/', batch_size=32):\n",
        "#     '''\n",
        "#     Probably a better way to do this using something like this:\n",
        "#     https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n",
        "    \n",
        "#     This loads a single random batch from the training set.\n",
        "#     '''\n",
        "#     pathlist = list(Path(directory).glob('**/*.jpeg'))\n",
        "#     labels = []\n",
        "#     images = []\n",
        "#     n_files = len(pathlist)\n",
        "#     random_indicies = [randint(0, n_files) for _ in range(batch_size)]\n",
        "\n",
        "#     for path in [pathlist[x] for x in random_indicies]:\n",
        "#         label = str(path).split(directory)[1].split(f'.')[0]\n",
        "#         label = onehot_from_fen(label)\n",
        "\n",
        "#         img = np.asarray(Image.open(str(path))).astype('uint8')\n",
        "#         labels.append(label)\n",
        "#         images.append(img)\n",
        "        \n",
        "#     test_images, test_labels = (images, labels) # TODO\n",
        "        \n",
        "#     return images, labels, test_images, test_labels\n",
        "\n",
        "\n",
        "# class CustomChessDataset(Dataset):\n",
        "#     \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "#     def __init__(self, transform=None, train=True):\n",
        "# #         train_images, train_labels, test_images, test_labels = load_datasets()\n",
        "#         self.transform = transform\n",
        "#         self.to_tensor = transforms.ToTensor()\n",
        "#         if train:\n",
        "#             self.images = train_images\n",
        "#             self.labels = np.asarray(train_labels)\n",
        "            \n",
        "#         else:\n",
        "#             self.images = test_images\n",
        "#             self.labels = np.asarray(test_labels)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.labels)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i = self.images[idx]\n",
        "#         img_as_img = Image.fromarray(i)\n",
        "#         img_as_img = img_as_img.convert('L')\n",
        "        \n",
        "# #         _,class_labels = torch.max(self.to_tensor(self.labels[idx]).long().to(device),1) \n",
        "\n",
        "#         return (self.to_tensor(img_as_img), \n",
        "#                 self.labels[idx])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ywR6A-Gepcy_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %matplotlib inline\n",
        "\n",
        "# # from matplotlib.pyplot import imshow\n",
        "# # import matplotlib.pyplot as plt\n",
        "# from draw_chess_boards import *\n",
        "\n",
        "# renderer = DrawChessPosition(delimiter='-')\n",
        "# fen = \"r2q1rk1/pp2ppbp/1np2np1/2Q3B1/3PP1b1/2N2N2/PP3PPP/3RKB1R\"\n",
        "# fen = \"rnbqkbnr-pppppppp-8-8-8-8-PPPPPPPP-RNBQKBNR\"\n",
        "# board = renderer.draw(fen)\n",
        "# renderer.show(board)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RjDdiqGMpczC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Sequential(\n",
        "#             nn.Conv2d(400*400*1, 32, 4),\n",
        "#             nn.BatchNorm2d(32),\n",
        "#             nn.MaxPool2d(2, 2),\n",
        "#             nn.Dropout(p = 0.1))\n",
        "#         self.conv2 = nn.Sequential(\n",
        "#             nn.Conv2d(400*400*1, 1024, 24),\n",
        "#             nn.BatchNorm2d(32),\n",
        "#             nn.MaxPool2d(12, 12),\n",
        "#             nn.Dropout(p = 0.1))\n",
        "#         self.fc1 = nn.Sequential(\n",
        "#             nn.Linear(1024, 64*52),\n",
        "#             nn.ReLU())\n",
        "#         self.fc2 = nn.Sequential(\n",
        "#             nn.Linear(64*52, 64*52),\n",
        "#             nn.ReLU())\n",
        "#         self.fc3 = nn.Sequential(\n",
        "#             nn.Linear(64*52, 64*26),\n",
        "#             nn.ReLU())\n",
        "#         self.fc4 = nn.Sequential(\n",
        "#             nn.Linear(64*26, 64*13),\n",
        "#             nn.ReLU())\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.view(x.shape[0], -1)\n",
        "#         out = self.conv1(x)\n",
        "#         out = self.fc1(out)\n",
        "#         out = self.fc2(out)\n",
        "#         out = self.fc3(out)\n",
        "#         out = self.fc4(out)\n",
        "#         out = out.reshape(1, 64, 13)\n",
        "# #         out = F.log_softmax(out, dim=2)\n",
        "\n",
        "# #         out = F.log_softmax(self.fc6(out.reshape(1, 64, 13)), dim=1)\n",
        "\n",
        "#         return out\n",
        "    \n",
        "    \n",
        "# # class CNN(nn.Module):\n",
        "# #     def __init__(self):\n",
        "# #         super(CNN, self).__init__()\n",
        "\n",
        "# #         self.conv1 = nn.Conv2d(3, 32, 4)\n",
        "# #         self.bn1 = nn.BatchNorm2d(32)\n",
        "# #         self.pool1 = nn.MaxPool2d(2, 2)\n",
        "# #         self.dropout1 = nn.Dropout(p = 0.1)\n",
        "\n",
        "# #         self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "# #         self.bn2 = nn.BatchNorm2d(64)\n",
        "# #         self.pool2 = nn.MaxPool2d(2, 2)\n",
        "# #         self.dropout2 = nn.Dropout(p = 0.2)\n",
        "\n",
        "# #         self.conv3 = nn.Conv2d(64, 128, 2)\n",
        "# #         self.bn3 = nn.BatchNorm2d(128)\n",
        "# #         self.pool3 = nn.MaxPool2d(2, 2)\n",
        "# #         self.dropout3 = nn.Dropout(p = 0.3)\n",
        "\n",
        "# #         self.conv4 = nn.Conv2d(128, 256, 3)\n",
        "# #         self.bn4 = nn.BatchNorm2d(256)\n",
        "# #         self.pool4 = nn.MaxPool2d(2, 2)\n",
        "# #         self.dropout4 = nn.Dropout(p = 0.4)\n",
        "\n",
        "# #         self.fc1 = nn.Linear(256*12*12, 1000)\n",
        "# #         self.dropout5 = nn.Dropout(p = 0.5)\n",
        "# #         self.fc2 = nn.Linear(1000, 1000)\n",
        "# #         self.dropout6 = nn.Dropout(p = 0.6)\n",
        "# #         self.fc3 = nn.Linear(1000, 250)\n",
        "# #         self.dropout7 = nn.Dropout(p = 0.7)\n",
        "# #         self.fc4 = nn.Linear(250, 120)\n",
        "\n",
        "\n",
        "# #     def forward(self, x):\n",
        "# #         x = self.dropout1(self.pool1(F.relu(self.bn1(self.conv1(x)))))\n",
        "# #         x = self.dropout2(self.pool2(F.relu(self.bn2(self.conv2(x)))))\n",
        "# #         x = self.dropout3(self.pool3(F.relu(self.bn3(self.conv3(x)))))\n",
        "# #         x = self.dropout4(self.pool4(F.relu(self.bn4(self.conv4(x)))))\n",
        "# #         x = x.view(x.size(0), -1)\n",
        "# #         x = self.dropout5(self.fc1(x))\n",
        "# #         x = self.dropout6(self.fc2(x))\n",
        "# #         x = self.dropout7(self.fc3(x))\n",
        "# #         x = self.fc4(x)\n",
        "# #         x = F.log_softmax(x, dim=1)\n",
        "# #         return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UATYG6EdpczE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def train(model, optimizer, loss_fn, dataloader, metrics, params):\n",
        "#     \"\"\"Train the model on `num_steps` batches\n",
        "\n",
        "#     Args:\n",
        "#         model: (torch.nn.Module) the neural network\n",
        "#         optimizer: (torch.optim) optimizer for parameters of model\n",
        "#         loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
        "#         dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
        "#         metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
        "#         params: (Params) hyperparameters\n",
        "#         num_steps: (int) number of batches to train on, each of size params.batch_size\n",
        "#     \"\"\"\n",
        "\n",
        "#     # set model to training mode\n",
        "#     model.train()\n",
        "\n",
        "#     # summary for current training loop and a running average object for loss\n",
        "#     summ = []\n",
        "#     loss_avg = RunningAverage()\n",
        "\n",
        "#     # Use tqdm for progress bar\n",
        "#     with tqdm(total=len(dataloader)) as t:\n",
        "#         for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
        "#             # move to GPU if available\n",
        "#             if params.cuda:\n",
        "#                 train_batch, labels_batch = train_batch.cuda(async=True), labels_batch.cuda(async=True)\n",
        "#             # convert to torch Variables\n",
        "#             train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
        "\n",
        "#             # compute model output and loss\n",
        "#             output_batch = model(train_batch)\n",
        "#             #logger.debug(\"train output_batch.shape = {}. labels_batch.shape = {}\".format(output_batch.shape, labels_batch.shape))\n",
        "\n",
        "#             # check if predictions are negative\n",
        "#             logger.info(\"negative predictions: {}\".format((output_batch < 0.0).any()))\n",
        "\n",
        "#             # compute loss\n",
        "#             loss = loss_fn(output_batch, labels_batch)\n",
        "#             logger.debug(\"loss: {}\".format(loss.data.item()))\n",
        "\n",
        "#             # clear previous gradients, compute gradients of all variables wrt loss\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "\n",
        "#             # performs updates using calculated gradients\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # Evaluate summaries only once in a while\n",
        "#             if i % params.save_summary_steps == 0:\n",
        "#                 # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
        "#                 output_batch = output_batch.data.cpu().numpy()\n",
        "#                 labels_batch = labels_batch.data.cpu().numpy()\n",
        "\n",
        "#                 # compute all metrics on this batch\n",
        "#                 summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
        "#                                  for metric in metrics}\n",
        "#                 summary_batch['loss'] = loss.data.item()\n",
        "#                 summ.append(summary_batch)\n",
        "\n",
        "#             # update the average loss\n",
        "#             loss_avg.update(loss.data.item())\n",
        "\n",
        "#             t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
        "#             t.update()\n",
        "\n",
        "#     # compute mean of all metrics in summary\n",
        "#     metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
        "#     metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
        "#     logger.info(\"- Train metrics: \" + metrics_string)\n",
        "\n",
        "\n",
        "# def train_and_evaluate(model, train_dataloader, val_dataloader, optimizer, loss_fn, metrics, params, model_dir,\n",
        "#                        restore_file=None):\n",
        "#     \"\"\"Train the model and evaluate every epoch.\n",
        "\n",
        "#     Args:\n",
        "#         model: (torch.nn.Module) the neural network\n",
        "#         train_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
        "#         val_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches validation data\n",
        "#         optimizer: (torch.optim) optimizer for parameters of model\n",
        "#         loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
        "#         metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
        "#         params: (Params) hyperparameters\n",
        "#         model_dir: (string) directory containing config, weights and log\n",
        "#         restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
        "#     \"\"\"\n",
        "\n",
        "#     # reload weights from restore_file if specified\n",
        "#     if restore_file is not None:\n",
        "#         restore_path = os.path.join(model_dir, restore_file + '.pth.tar')\n",
        "#         logger.info(\"Restoring parameters from {}\".format(restore_path))\n",
        "#         load_checkpoint(restore_path, model, optimizer)\n",
        "\n",
        "#     best_val_acc = 0.0\n",
        "\n",
        "#     for epoch in range(params.num_epochs):\n",
        "#         # Run one epoch\n",
        "#         logger.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
        "\n",
        "#         # compute number of batches in one epoch (one full pass over the training set)\n",
        "#         train(model, optimizer, loss_fn, train_dataloader, metrics=metrics, params=params)\n",
        "\n",
        "#         # Evaluate for one epoch on validation set\n",
        "#         val_metrics = evaluate(model, loss_fn, val_dataloader, metrics=metrics, params=params)\n",
        "\n",
        "#         # TODO: Fix TypeError: 'NoneType' object is not subscriptable\n",
        "#         val_acc = val_metrics['accuracy']\n",
        "#         is_best = val_acc>=best_val_acc\n",
        "\n",
        "#         # Save weights\n",
        "#         save_checkpoint({'epoch': epoch + 1,\n",
        "#                          'state_dict': model.state_dict(),\n",
        "#                          'optim_dict' : optimizer.state_dict()},\n",
        "#                           is_best=is_best,\n",
        "#                           checkpoint=model_dir)\n",
        "\n",
        "#         # If best_eval, best_save_path\n",
        "#         if is_best:\n",
        "#             logger.info(\"- Found new best accuracy\")\n",
        "#             best_val_acc = val_acc\n",
        "\n",
        "#             # Save best val metrics in a json file in the model directory\n",
        "#             best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
        "#             save_dict_to_json(val_metrics, best_json_path)\n",
        "\n",
        "#         # Save latest val metrics in a json file in the model directory\n",
        "#         last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
        "#         save_dict_to_json(val_metrics, last_json_path)\n",
        "\n",
        "# def main():\n",
        "#     # print some log messages\n",
        "#     logger.info(\"DSTL Satellite Imagery Feature Detection - Train U-Net Model\")\n",
        "\n",
        "#     # load parameters\n",
        "#     # load parameters from configuration file\n",
        "#     params = Params('experiment/unet_model/params_3ch.yaml', ParameterFileType.YAML, ctx=None)\n",
        "\n",
        "#     # parameters\n",
        "#     logger.debug(\"parameters: \\n{}\\n\".format(pformat(params.dict)))\n",
        "\n",
        "#     # use GPU if available\n",
        "#     params.cuda = torch.cuda.is_available()\n",
        "\n",
        "#     # Set the random seed for reproducible experiments\n",
        "#     torch.manual_seed(230)\n",
        "#     if params.cuda:\n",
        "#         torch.cuda.manual_seed(230)\n",
        "\n",
        "#     # dataset parameters, which includes download, input, output and mask generation parameters.\n",
        "#     dataset_params = params.dataset\n",
        "#     logger.debug(\"dataset parameters: \\n{}\\n\".format(pformat(dataset_params)))\n",
        "\n",
        "#     # dataset\n",
        "#     logger.info(\"loading datasets...\")\n",
        "#     train_set = DSTLSIFDDataset(dataset_params=dataset_params,\n",
        "#                                 mode='train',\n",
        "#                                 transform=True,\n",
        "#                                 transform_mask=None,\n",
        "#                                 download=False)\n",
        "\n",
        "#     dev_set   = DSTLSIFDDataset(dataset_params=dataset_params,\n",
        "#                                 mode='dev',\n",
        "#                                 transform=True,\n",
        "#                                 transform_mask=None,\n",
        "#                                 download=False)\n",
        "\n",
        "#     # dataloader\n",
        "#     logger.debug(\"train dataloader, batch size: {}, num workers: {}, cuda: {}\".format(\n",
        "#         params.train['batch_size'],\n",
        "#         params.train['num_workers'],\n",
        "#         params.cuda));\n",
        "\n",
        "#     train_dl = DataLoader(dataset=train_set,\n",
        "#                           batch_size=params.train['batch_size'],\n",
        "#                           shuffle=True,\n",
        "#                           num_workers=params.train['num_workers'],\n",
        "#                           pin_memory=params.cuda)\n",
        "\n",
        "#     logger.debug(\"dev dataloader, batch size: {}, num workers: {}, cuda: {}\".format(\n",
        "#         params.valid['batch_size'],\n",
        "#         params.valid['num_workers'],\n",
        "#         params.cuda));\n",
        "\n",
        "#     valid_dl = DataLoader(dataset=dev_set,\n",
        "#                           batch_size=params.valid['batch_size'],\n",
        "#                           shuffle=True,\n",
        "#                           num_workers=params.valid['num_workers'],\n",
        "#                           pin_memory=params.cuda)\n",
        "\n",
        "#     logger.info(\"- done.\")\n",
        "\n",
        "#     # define the model and optimizer\n",
        "#     #model = UNet()\n",
        "#     model = UNet().cuda() if params.cuda else UNet()\n",
        "#     logger.info(\"using adam optimized with lr = {}\".format(float(params.learning_rate)))\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=float(params.learning_rate))\n",
        "\n",
        "#     # loss function\n",
        "#     loss_fn = multi_class_cross_entropy_loss  # nn.MSELoss()  # nn.L1Loss() # nn.CrossEntropyLoss()\n",
        "\n",
        "#     # maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
        "#     metrics = {\n",
        "#         'accuracy': accuracy,\n",
        "#         # could add more metrics such as accuracy for each token type\n",
        "#     }\n",
        "\n",
        "#     # train the model\n",
        "#     logger.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
        "\n",
        "#     data_dir = \"data/\"\n",
        "#     model_dir = \"experiment/unet_model\"\n",
        "\n",
        "#     train_and_evaluate(model=model,\n",
        "#                        train_dataloader=train_dl,\n",
        "#                        val_dataloader=valid_dl,\n",
        "#                        optimizer=optimizer,\n",
        "#                        loss_fn=loss_fn,\n",
        "#                        metrics=metrics,\n",
        "#                        params=params,\n",
        "#                        model_dir=data_dir,\n",
        "#                        restore_file=None)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aeo0yvCSTE89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Adapted from here: https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n",
        "\n",
        "# import torch.utils.data as data\n",
        "\n",
        "# from PIL import Image\n",
        "\n",
        "# import os\n",
        "# import os.path\n",
        "# import sys\n",
        "\n",
        "\n",
        "# def has_file_allowed_extension(filename, extensions):\n",
        "#     \"\"\"Checks if a file is an allowed extension.\n",
        "#     Args:\n",
        "#         filename (string): path to a file\n",
        "#         extensions (iterable of strings): extensions to consider (lowercase)\n",
        "#     Returns:\n",
        "#         bool: True if the filename ends with one of given extensions\n",
        "#     \"\"\"\n",
        "#     filename_lower = filename.lower()\n",
        "#     return any(filename_lower.endswith(ext) for ext in extensions)\n",
        "\n",
        "\n",
        "# def is_image_file(filename):\n",
        "#     \"\"\"Checks if a file is an allowed image extension.\n",
        "#     Args:\n",
        "#         filename (string): path to a file\n",
        "#     Returns:\n",
        "#         bool: True if the filename ends with a known image extension\n",
        "#     \"\"\"\n",
        "#     return has_file_allowed_extension(filename, IMG_EXTENSIONS)\n",
        "\n",
        "\n",
        "# def make_dataset(dir, class_to_idx, extensions):\n",
        "#     images = []\n",
        "#     dir = os.path.expanduser(dir)\n",
        "#     for target in sorted(class_to_idx.keys()):\n",
        "#         d = os.path.join(dir, target)\n",
        "#         if not os.path.isdir(d):\n",
        "#             continue\n",
        "\n",
        "#         for root, _, fnames in sorted(os.walk(d)):\n",
        "#             for fname in sorted(fnames):\n",
        "#                 if has_file_allowed_extension(fname, extensions):\n",
        "#                     path = os.path.join(root, fname)\n",
        "#                     item = (path, class_to_idx[target])\n",
        "#                     images.append(item)\n",
        "\n",
        "#     return images\n",
        "\n",
        "\n",
        "# class DatasetFolder(data.Dataset):\n",
        "#     \"\"\"A generic data loader where the samples are arranged in this way: ::\n",
        "#         root/class_x/xxx.ext\n",
        "#         root/class_x/xxy.ext\n",
        "#         root/class_x/xxz.ext\n",
        "#         root/class_y/123.ext\n",
        "#         root/class_y/nsdf3.ext\n",
        "#         root/class_y/asd932_.ext\n",
        "#     Args:\n",
        "#         root (string): Root directory path.\n",
        "#         loader (callable): A function to load a sample given its path.\n",
        "#         extensions (list[string]): A list of allowed extensions.\n",
        "#         transform (callable, optional): A function/transform that takes in\n",
        "#             a sample and returns a transformed version.\n",
        "#             E.g, ``transforms.RandomCrop`` for images.\n",
        "#         target_transform (callable, optional): A function/transform that takes\n",
        "#             in the target and transforms it.\n",
        "#      Attributes:\n",
        "#         classes (list): List of the class names.\n",
        "#         class_to_idx (dict): Dict with items (class_name, class_index).\n",
        "#         samples (list): List of (sample path, class_index) tuples\n",
        "#         targets (list): The class_index value for each image in the dataset\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, root, loader, extensions, transform=None, target_transform=None):\n",
        "#         classes, class_to_idx = self._find_classes(root)\n",
        "#         samples = make_dataset(root, class_to_idx, extensions)\n",
        "#         if len(samples) == 0:\n",
        "#             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n",
        "#                                \"Supported extensions are: \" + \",\".join(extensions)))\n",
        "\n",
        "#         self.root = root\n",
        "#         self.loader = loader\n",
        "#         self.extensions = extensions\n",
        "\n",
        "#         self.classes = classes\n",
        "#         self.class_to_idx = class_to_idx\n",
        "#         self.samples = samples\n",
        "#         self.targets = [s[1] for s in samples]\n",
        "\n",
        "#         self.transform = transform\n",
        "#         self.target_transform = target_transform\n",
        "\n",
        "#     def _find_classes(self, dir):\n",
        "#         \"\"\"\n",
        "#         Finds the class folders in a dataset.\n",
        "#         Args:\n",
        "#             dir (string): Root directory path.\n",
        "#         Returns:\n",
        "#             tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
        "#         Ensures:\n",
        "#             No class is a subdirectory of another.\n",
        "#         \"\"\"\n",
        "#         if sys.version_info >= (3, 5):\n",
        "#             # Faster and available in Python 3.5 and above\n",
        "#             classes = [d.name.split('.jpeg')[0] for d in os.scandir(dir) if not d.is_dir()]\n",
        "#         else:\n",
        "#             classes = [d.name.split('.jpeg')[0] for d in os.listdir(dir) if not os.path.isdir(os.path.join(dir, d))]\n",
        "#         classes.sort()\n",
        "#         class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "#         return classes, class_to_idx\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             index (int): Index\n",
        "#         Returns:\n",
        "#             tuple: (sample, target) where target is class_index of the target class.\n",
        "#         \"\"\"\n",
        "#         path, target = self.samples[index]\n",
        "#         sample = self.loader(path)\n",
        "#         if self.transform is not None:\n",
        "#             sample = self.transform(sample)\n",
        "#         if self.target_transform is not None:\n",
        "#             target = self.target_transform(target)\n",
        "\n",
        "#         return sample, target\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.samples)\n",
        "\n",
        "#     def __repr__(self):\n",
        "#         fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "#         fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "#         fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "#         tmp = '    Transforms (if any): '\n",
        "#         fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "#         tmp = '    Target Transforms (if any): '\n",
        "#         fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "#         return fmt_str\n",
        "\n",
        "\n",
        "# IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', 'webp']\n",
        "\n",
        "\n",
        "# def pil_loader(path):\n",
        "#     # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "#     with open(path, 'rb') as f:\n",
        "#         img = Image.open(f)\n",
        "#         return img.convert('RGB')\n",
        "\n",
        "\n",
        "# def accimage_loader(path):\n",
        "#     import accimage\n",
        "#     try:\n",
        "#         return accimage.Image(path)\n",
        "#     except IOError:\n",
        "#         # Potentially a decoding problem, fall back to PIL.Image\n",
        "#         return pil_loader(path)\n",
        "\n",
        "\n",
        "# def default_loader(path):\n",
        "#     from torchvision import get_image_backend\n",
        "#     if get_image_backend() == 'accimage':\n",
        "#         return accimage_loader(path)\n",
        "#     else:\n",
        "#         return pil_loader(path)\n",
        "\n",
        "\n",
        "# class ImageFolder(DatasetFolder):\n",
        "#     \"\"\"A generic data loader where the images are arranged in this way: ::\n",
        "#         root/dog/xxx.png\n",
        "#         root/dog/xxy.png\n",
        "#         root/dog/xxz.png\n",
        "#         root/cat/123.png\n",
        "#         root/cat/nsdf3.png\n",
        "#         root/cat/asd932_.png\n",
        "#     Args:\n",
        "#         root (string): Root directory path.\n",
        "#         transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "#             and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "#         target_transform (callable, optional): A function/transform that takes in the\n",
        "#             target and transforms it.\n",
        "#         loader (callable, optional): A function to load an image given its path.\n",
        "#      Attributes:\n",
        "#         classes (list): List of the class names.\n",
        "#         class_to_idx (dict): Dict with items (class_name, class_index).\n",
        "#         imgs (list): List of (image path, class_index) tuples\n",
        "#     \"\"\"\n",
        "#     def __init__(self, root, transform=None, target_transform=None,\n",
        "#                  loader=default_loader):\n",
        "#         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n",
        "#                                           transform=transform,\n",
        "#                                           target_transform=target_transform)\n",
        "#         self.imgs = self.samples\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}